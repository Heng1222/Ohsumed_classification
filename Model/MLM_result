Mounted at /content/drive
label	title
0	0	Haemophilus influenzae meningitis with prolong...
1	0	Augmentation mentoplasty using Mersilene mesh.
2	0	Multiple intracranial mucoceles associated wit...
3	0	Replacement of an aortic valve cusp after neon...
4	0	Mucosal intussusception to avoid ascending cho...
每個類別將統一採樣至: 427 筆
train data size:  (8838, 2)
test data size:  (983, 2)
Running Experiment with Custom RoBERTa...
/tmp/ipython-input-4251761289.py:177: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  full_df = full_df.groupby('label').apply(
Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/MyDrive/roberta_ohsumed_mlm and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
base model：

 =====================================================================================
Layer (type:depth-idx)                                       Param #
=====================================================================================
RobertaModel                                                 --
├─RobertaEmbeddings: 1-1                                     --
│    └─Embedding: 2-1                                        (38,603,520)
│    └─Embedding: 2-2                                        (394,752)
│    └─Embedding: 2-3                                        (768)
│    └─LayerNorm: 2-4                                        (1,536)
│    └─Dropout: 2-5                                          --
├─RobertaEncoder: 1-2                                        --
│    └─ModuleList: 2-6                                       --
│    │    └─RobertaLayer: 3-1                                (7,087,872)
│    │    └─RobertaLayer: 3-2                                (7,087,872)
│    │    └─RobertaLayer: 3-3                                (7,087,872)
│    │    └─RobertaLayer: 3-4                                (7,087,872)
│    │    └─RobertaLayer: 3-5                                (7,087,872)
│    │    └─RobertaLayer: 3-6                                (7,087,872)
│    │    └─RobertaLayer: 3-7                                (7,087,872)
│    │    └─RobertaLayer: 3-8                                (7,087,872)
│    │    └─RobertaLayer: 3-9                                (7,087,872)
│    │    └─RobertaLayer: 3-10                               (7,087,872)
│    │    └─RobertaLayer: 3-11                               (7,087,872)
│    │    └─RobertaLayer: 3-12                               (7,087,872)
├─RobertaPooler: 1-3                                         --
│    └─Linear: 2-7                                           (590,592)
│    └─Tanh: 2-8                                             --
=====================================================================================
Total params: 124,645,632
Trainable params: 0
Non-trainable params: 124,645,632
=====================================================================================
classifer NN Head：

 =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Sequential                               --
├─Linear: 1-1                            17,687
=================================================================
Total params: 17,687
Trainable params: 17,687
Non-trainable params: 0
=================================================================
Epoch 1: 100%|██████████| 249/249 [03:58<00:00,  1.05it/s]
Epoch 1 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.20it/s]
Epoch 1 Training Loss: 3.1151 Validation Loss: 3.0782
Epoch 2: 100%|██████████| 249/249 [04:07<00:00,  1.01it/s]
Epoch 2 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 2 Training Loss: 3.0520 Validation Loss: 3.0218
Epoch 3: 100%|██████████| 249/249 [04:06<00:00,  1.01it/s]
Epoch 3 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 3 Training Loss: 3.0010 Validation Loss: 2.9710
Epoch 4: 100%|██████████| 249/249 [04:06<00:00,  1.01it/s]
Epoch 4 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 4 Training Loss: 2.9506 Validation Loss: 2.9244
Epoch 5: 100%|██████████| 249/249 [04:07<00:00,  1.01it/s]
Epoch 5 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 5 Training Loss: 2.9114 Validation Loss: 2.8820
Epoch 6: 100%|██████████| 249/249 [04:06<00:00,  1.01it/s]
Epoch 6 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 6 Training Loss: 2.8716 Validation Loss: 2.8439
Epoch 7: 100%|██████████| 249/249 [04:06<00:00,  1.01it/s]
Epoch 7 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 7 Training Loss: 2.8418 Validation Loss: 2.8092
Epoch 8: 100%|██████████| 249/249 [04:06<00:00,  1.01it/s]
Epoch 8 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 8 Training Loss: 2.8112 Validation Loss: 2.7769
Epoch 9: 100%|██████████| 249/249 [04:06<00:00,  1.01it/s]
Epoch 9 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 9 Training Loss: 2.7851 Validation Loss: 2.7481
Epoch 10: 100%|██████████| 249/249 [04:06<00:00,  1.01it/s]
Epoch 10 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 10 Training Loss: 2.7602 Validation Loss: 2.7201
Epoch 11: 100%|██████████| 249/249 [04:06<00:00,  1.01it/s]
Epoch 11 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 11 Training Loss: 2.7395 Validation Loss: 2.6960
Epoch 12: 100%|██████████| 249/249 [04:07<00:00,  1.01it/s]
Epoch 12 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 12 Training Loss: 2.7226 Validation Loss: 2.6729
Epoch 13: 100%|██████████| 249/249 [04:07<00:00,  1.01it/s]
Epoch 13 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 13 Training Loss: 2.6992 Validation Loss: 2.6513
Epoch 14: 100%|██████████| 249/249 [04:07<00:00,  1.01it/s]
Epoch 14 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 14 Training Loss: 2.6766 Validation Loss: 2.6300
Epoch 15: 100%|██████████| 249/249 [04:07<00:00,  1.01it/s]
Epoch 15 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 15 Training Loss: 2.6655 Validation Loss: 2.6103
Epoch 16: 100%|██████████| 249/249 [04:07<00:00,  1.01it/s]
Epoch 16 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 16 Training Loss: 2.6451 Validation Loss: 2.5921
Epoch 17: 100%|██████████| 249/249 [04:06<00:00,  1.01it/s]
Epoch 17 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 17 Training Loss: 2.6294 Validation Loss: 2.5764
Epoch 18: 100%|██████████| 249/249 [04:06<00:00,  1.01it/s]
Epoch 18 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 18 Training Loss: 2.6241 Validation Loss: 2.5602
Epoch 19: 100%|██████████| 249/249 [04:07<00:00,  1.01it/s]
Epoch 19 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 19 Training Loss: 2.6101 Validation Loss: 2.5466
Epoch 20: 100%|██████████| 249/249 [04:07<00:00,  1.01it/s]
Epoch 20 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 20 Training Loss: 2.6006 Validation Loss: 2.5327
Epoch 21: 100%|██████████| 249/249 [04:06<00:00,  1.01it/s]
Epoch 21 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 21 Training Loss: 2.5795 Validation Loss: 2.5198
Epoch 22: 100%|██████████| 249/249 [04:07<00:00,  1.01it/s]
Epoch 22 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 22 Training Loss: 2.5687 Validation Loss: 2.5055
Epoch 23: 100%|██████████| 249/249 [04:06<00:00,  1.01it/s]
Epoch 23 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 23 Training Loss: 2.5608 Validation Loss: 2.4935
Epoch 24: 100%|██████████| 249/249 [04:07<00:00,  1.01it/s]
Epoch 24 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 24 Training Loss: 2.5544 Validation Loss: 2.4839
Epoch 25: 100%|██████████| 249/249 [04:07<00:00,  1.01it/s]
Epoch 25 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 25 Training Loss: 2.5491 Validation Loss: 2.4715
Epoch 26: 100%|██████████| 249/249 [04:06<00:00,  1.01it/s]
Epoch 26 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 26 Training Loss: 2.5471 Validation Loss: 2.4639
Epoch 27: 100%|██████████| 249/249 [04:06<00:00,  1.01it/s]
Epoch 27 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 27 Training Loss: 2.5303 Validation Loss: 2.4536
Epoch 28: 100%|██████████| 249/249 [04:06<00:00,  1.01it/s]
Epoch 28 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 28 Training Loss: 2.5332 Validation Loss: 2.4440
Epoch 29: 100%|██████████| 249/249 [04:06<00:00,  1.01it/s]
Epoch 29 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 29 Training Loss: 2.5197 Validation Loss: 2.4363
Epoch 30: 100%|██████████| 249/249 [04:06<00:00,  1.01it/s]
Epoch 30 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 30 Training Loss: 2.5145 Validation Loss: 2.4274
Epoch 31: 100%|██████████| 249/249 [04:06<00:00,  1.01it/s]
Epoch 31 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 31 Training Loss: 2.5070 Validation Loss: 2.4194
Epoch 32: 100%|██████████| 249/249 [04:07<00:00,  1.01it/s]
Epoch 32 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 32 Training Loss: 2.4908 Validation Loss: 2.4102
Epoch 33: 100%|██████████| 249/249 [04:06<00:00,  1.01it/s]
Epoch 33 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 33 Training Loss: 2.4968 Validation Loss: 2.4037
Epoch 34: 100%|██████████| 249/249 [04:06<00:00,  1.01it/s]
Epoch 34 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 34 Training Loss: 2.4868 Validation Loss: 2.3963
Epoch 35: 100%|██████████| 249/249 [04:06<00:00,  1.01it/s]
Epoch 35 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 35 Training Loss: 2.4894 Validation Loss: 2.3906
Epoch 36: 100%|██████████| 249/249 [04:07<00:00,  1.01it/s]
Epoch 36 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 36 Training Loss: 2.4756 Validation Loss: 2.3830
Epoch 37: 100%|██████████| 249/249 [04:06<00:00,  1.01it/s]
Epoch 37 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 37 Training Loss: 2.4670 Validation Loss: 2.3758
Epoch 38: 100%|██████████| 249/249 [04:06<00:00,  1.01it/s]
Epoch 38 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 38 Training Loss: 2.4611 Validation Loss: 2.3701
Epoch 39: 100%|██████████| 249/249 [04:07<00:00,  1.01it/s]
Epoch 39 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 39 Training Loss: 2.4555 Validation Loss: 2.3666
Epoch 40: 100%|██████████| 249/249 [04:07<00:00,  1.01it/s]
Epoch 40 Validation: 100%|██████████| 28/28 [00:23<00:00,  1.19it/s]
Epoch 40 Training Loss: 2.4547 Validation Loss: 2.3585

--- Final Evaluation Report ---
              precision    recall  f1-score   support

           0       0.22      0.24      0.23        42
           1       0.39      0.60      0.47        43
           2       0.38      0.44      0.41        43
           3       0.24      0.49      0.32        43
           4       0.32      0.47      0.38        43
           5       0.60      0.35      0.44        43
           6       0.44      0.28      0.34        43
           7       0.31      0.26      0.28        43
           8       0.46      0.50      0.48        42
           9       0.24      0.14      0.18        42
          10       0.40      0.49      0.44        43
          11       0.31      0.19      0.23        43
          12       0.43      0.55      0.48        42
          13       0.31      0.42      0.36        43
          14       0.28      0.19      0.22        43
          15       0.37      0.36      0.36        42
          16       0.38      0.28      0.32        43
          17       0.44      0.62      0.51        42
          18       0.31      0.19      0.23        43
          19       0.26      0.14      0.18        43
          20       0.36      0.51      0.42        43
          21       0.54      0.58      0.56        43
          22       0.00      0.00      0.00        43

    accuracy                           0.36       983
   macro avg       0.35      0.36      0.34       983
weighted avg       0.35      0.36      0.34       983

