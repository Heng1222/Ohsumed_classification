{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "10NodytfPbXYcGFRy4eNfCHftNvWyUunw",
      "authorship_tag": "ABX9TyPtWo1HTtQ8Fn3ybUW1Z7Z2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Heng1222/Ohsumed_classification/blob/main/Model/task3_MeSH_LoRA_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq-NNk_9nIZN",
        "outputId": "3b907ef1-3cb7-478b-c10f-51aa6bdbb0bd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wayYZ2l-fkeC",
        "outputId": "e1a42095-0628-4c0f-d53a-dee4ba72a927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===============================================================================================\n",
            "Layer (type:depth-idx)                                                 Param #\n",
            "===============================================================================================\n",
            "PeftModel                                                              --\n",
            "├─LoraModel: 1-1                                                       --\n",
            "│    └─RobertaModel: 2-1                                               --\n",
            "│    │    └─RobertaEmbeddings: 3-1                                     (39,000,576)\n",
            "│    │    └─RobertaEncoder: 3-2                                        86,971,392\n",
            "│    │    └─RobertaPooler: 3-3                                         (590,592)\n",
            "===============================================================================================\n",
            "Total params: 126,562,560\n",
            "Trainable params: 1,916,928\n",
            "Non-trainable params: 124,645,632\n",
            "===============================================================================================\n",
            "test size: (654, 3)\n",
            "train size: (5882, 3)\n",
            "                          word_i                               word_j  \\\n",
            "78124            Endophthalmitis                            Typhlitis   \n",
            "120793    Diabetic Nephropathies  Benign Optic Nerve Sheath Neoplasms   \n",
            "1841    Sebaceous Gland Diseases                     LEOPARD Syndrome   \n",
            "43449   Hereditary Hyperekplexia                         Folliculitis   \n",
            "49227             Pneumoconiosis            Myasthenia Gravis, Ocular   \n",
            "\n",
            "        wup_similarity  \n",
            "78124             0.20  \n",
            "120793            0.22  \n",
            "1841              0.20  \n",
            "43449             0.20  \n",
            "49227             0.18  \n",
            "                                    word_i  \\\n",
            "93056                Proteus-Like Syndrome   \n",
            "127298                     Facial Neuritis   \n",
            "117232                       Ovarian Cysts   \n",
            "25324   Complex Partial Status Epilepticus   \n",
            "90954   Sphenoid Allergic Fungal Sinusitis   \n",
            "\n",
            "                                                 word_j  wup_similarity  \n",
            "93056   Anti-N-Methyl-D-Aspartate Receptor Encephalitis            0.20  \n",
            "127298                            Hemorrhagic Disorders            0.25  \n",
            "117232                                Humeral Fractures            0.22  \n",
            "25324                         Dermatitis, Toxicodendron            0.17  \n",
            "90954                                     Shock, Septic            0.20  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Training:   1%|          | 2/184 [01:07<1:41:52, 33.59s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-793614925.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;31m# 執行\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mesh_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-793614925.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(csv_path)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wup_sim\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "from torch.optim import AdamW\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import matplotlib.pyplot as plt\n",
        "from torchinfo import summary\n",
        "from tqdm import tqdm # Import tqdm for progress bar\n",
        "\n",
        "# 1. 定義論文中的 MeSH Semantic Loss (WSL)\n",
        "# 公式: L_WSL = (1/|P|) * sum(|CosSim(v_i, v_j) - WUP(s_i, s_j)|)^2\n",
        "class MeSHSemanticLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MeSHSemanticLoss, self).__init__()\n",
        "\n",
        "    def forward(self, vec_i, vec_j, target_wup):\n",
        "        # 計算餘弦相似度 CosSim(v_i, v_j)\n",
        "        cos_sim = F.cosine_similarity(vec_i, vec_j)\n",
        "        # 計算誤差 e_WSL = |CosSim - WUP|\n",
        "        loss = torch.mean((cos_sim - target_wup) ** 2)\n",
        "        return loss\n",
        "\n",
        "# 2. 定義資料集類別，用於處理 CSV 讀取的資料\n",
        "class SemanticPairDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.encoded_i = tokenizer(dataframe['word_i'].tolist(), padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "        self.encoded_j = tokenizer(dataframe['word_j'].tolist(), padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "        self.wup_sim = torch.tensor(dataframe['wup_similarity'].values.astype(float), dtype=torch.float)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.wup_sim)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids_i\": self.encoded_i[\"input_ids\"][idx],\n",
        "            \"attention_mask_i\": self.encoded_i[\"attention_mask\"][idx],\n",
        "            \"input_ids_j\": self.encoded_j[\"input_ids\"][idx],\n",
        "            \"attention_mask_j\": self.encoded_j[\"attention_mask\"][idx],\n",
        "            \"wup_sim\": self.wup_sim[idx]\n",
        "        }\n",
        "\n",
        "def run_training(csv_path):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 初始化模型與 Tokenizer\n",
        "    model_name = \"roberta-base\"\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "    base_model = RobertaModel.from_pretrained(model_name)\n",
        "\n",
        "    # 4. LoRA 配置 (r=16, alpha=32)\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        target_modules=[\"query\", \"key\", \"value\", \"output.dense\"],\n",
        "        lora_dropout=0.1,\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(base_model, lora_config)\n",
        "    model.to(device)\n",
        "\n",
        "    # 5. 資料切分 (9:1 訓練集/測試集)\n",
        "    full_df = pd.read_csv(csv_path)\n",
        "    full_df = full_df.sample(frac=0.05, random_state=42) # 減少資料量以加速訓練和除錯\n",
        "    train_df = full_df.sample(frac=0.9, random_state=42) # 90% 訓練集\n",
        "    test_df = full_df.drop(train_df.index)               # 10% 測試集\n",
        "\n",
        "    # ==================================\n",
        "    print(summary(model))\n",
        "    print(f\"test size:\", test_df.shape)\n",
        "    print(f\"train size:\", train_df.shape)\n",
        "    print(train_df.head())\n",
        "    print(test_df.head())\n",
        "    # ==================================\n",
        "\n",
        "    train_loader = DataLoader(SemanticPairDataset(train_df, tokenizer=tokenizer), batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(SemanticPairDataset(test_df, tokenizer=tokenizer), batch_size=32)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "    criterion = MeSHSemanticLoss()\n",
        "\n",
        "    # 紀錄 Loss 用於輸出圖表\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    # Early Stopping 參數\n",
        "    patience = 3\n",
        "    min_delta = 0.0001 # 最小改善幅度\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    # 6. 訓練迴圈\n",
        "    num_epochs = 20 # 增加 epochs 數量以配合 Early Stopping\n",
        "    for epoch in range(num_epochs):\n",
        "      model.train() # 訓練模式\n",
        "      total_train_loss = 0\n",
        "      for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 直接從 batch 獲取資料並移至 device\n",
        "        emb_i = model(input_ids=batch[\"input_ids_i\"].to(device),\n",
        "                      attention_mask=batch[\"attention_mask_i\"].to(device)).last_hidden_state[:, 0, :]\n",
        "        emb_j = model(input_ids=batch[\"input_ids_j\"].to(device),\n",
        "                      attention_mask=batch[\"attention_mask_j\"].to(device)).last_hidden_state[:, 0, :]\n",
        "\n",
        "        loss = criterion(emb_i, emb_j, batch[\"wup_sim\"].to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "      # 驗證階段修正\n",
        "      model.eval() # 重要：關閉 Dropout\n",
        "      total_val_loss = 0\n",
        "      with torch.no_grad():\n",
        "          for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n",
        "              emb_i = model(input_ids=batch[\"input_ids_i\"].to(device),\n",
        "                            attention_mask=batch[\"attention_mask_i\"].to(device)).last_hidden_state[:, 0, :]\n",
        "              emb_j = model(input_ids=batch[\"input_ids_j\"].to(device),\n",
        "                            attention_mask=batch[\"attention_mask_j\"].to(device)).last_hidden_state[:, 0, :]\n",
        "              total_val_loss += criterion(emb_i, emb_j, batch[\"wup_sim\"].to(device)).item()\n",
        "\n",
        "      avg_train = total_train_loss / len(train_loader)\n",
        "      avg_val = total_val_loss / len(val_loader)\n",
        "      train_losses.append(avg_train)\n",
        "      val_losses.append(avg_val)\n",
        "\n",
        "      print(f\"Epoch {epoch+1}: Train Loss={avg_train:.6f}, Test Loss={avg_val:.6f}\")\n",
        "\n",
        "      # Early Stopping 檢查\n",
        "      if avg_val < best_val_loss - min_delta:\n",
        "          best_val_loss = avg_val\n",
        "          patience_counter = 0\n",
        "          # 可以選擇在這裡保存最佳模型\n",
        "          model.save_pretrained(\"roberta_semantic_lora_best\")\n",
        "      else:\n",
        "          patience_counter += 1\n",
        "          print(f\"Early Stopping: Validation loss has not improved for {patience_counter} epochs.\")\n",
        "          if patience_counter >= patience:\n",
        "              print(f\"Early stopping triggered after {epoch+1} epochs. Best validation loss: {best_val_loss:.6f}\")\n",
        "              break\n",
        "\n",
        "    # 7. 繪製 Loss 曲線 [cite: 624]\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Val Loss')\n",
        "    plt.title('Training and Validation Semantic Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # 儲存 LoRA 適配器 [cite: 252, 474]\n",
        "    model.save_pretrained(\"roberta_semantic_lora\")\n",
        "    print(\"LoRA Adapter saved to 'roberta_semantic_lora'.\")\n",
        "# 執行\n",
        "if __name__ == \"__main__\":\n",
        "    run_training('mesh_dataset.csv')\n"
      ]
    }
  ]
}