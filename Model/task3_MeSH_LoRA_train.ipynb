{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Heng1222/Ohsumed_classification/blob/main/Model/task3_MeSH_LoRA_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq-NNk_9nIZN",
        "outputId": "3e496ec1-64f4-42ef-f939-dd9f745532e3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wayYZ2l-fkeC",
        "outputId": "44530740-0eb3-44f4-dd67-ab7ae84d03c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===============================================================================================\n",
            "Layer (type:depth-idx)                                                 Param #\n",
            "===============================================================================================\n",
            "PeftModel                                                              --\n",
            "├─LoraModel: 1-1                                                       --\n",
            "│    └─RobertaModel: 2-1                                               --\n",
            "│    │    └─RobertaEmbeddings: 3-1                                     (39,000,576)\n",
            "│    │    └─RobertaEncoder: 3-2                                        86,971,392\n",
            "│    │    └─RobertaPooler: 3-3                                         (590,592)\n",
            "===============================================================================================\n",
            "Total params: 126,562,560\n",
            "Trainable params: 1,916,928\n",
            "Non-trainable params: 124,645,632\n",
            "===============================================================================================\n",
            "test size: (600, 3)\n",
            "train size: (5400, 3)\n",
            "       wup_similarity\n",
            "count     5400.000000\n",
            "mean         0.200246\n",
            "std          0.077916\n",
            "min          0.110000\n",
            "25%          0.170000\n",
            "50%          0.180000\n",
            "75%          0.200000\n",
            "max          1.000000\n",
            "       wup_similarity\n",
            "count      600.000000\n",
            "mean         0.200333\n",
            "std          0.076369\n",
            "min          0.120000\n",
            "25%          0.170000\n",
            "50%          0.180000\n",
            "75%          0.200000\n",
            "max          0.670000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Training: 100%|██████████| 169/169 [00:41<00:00,  4.05it/s]\n",
            "Epoch 1 Validation: 100%|██████████| 19/19 [00:01<00:00,  9.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss=0.142182, Test Loss=0.014496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Training: 100%|██████████| 169/169 [00:40<00:00,  4.20it/s]\n",
            "Epoch 2 Validation: 100%|██████████| 19/19 [00:01<00:00, 10.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss=0.010518, Test Loss=0.013138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Training:  33%|███▎      | 55/169 [00:13<00:27,  4.17it/s]"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "from torch.optim import AdamW\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import matplotlib.pyplot as plt\n",
        "from torchinfo import summary\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. 定義論文中的 MeSH Semantic Loss (WSL)\n",
        "# 公式: L_WSL = (1/|P|) * sum(|CosSim(v_i, v_j) - WUP(s_i, s_j)|)^2\n",
        "class MeSHSemanticLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MeSHSemanticLoss, self).__init__()\n",
        "\n",
        "    def forward(self, vec_i, vec_j, target_wup):\n",
        "        # 計算餘弦相似度 CosSim(v_i, v_j)\n",
        "        cos_sim = F.cosine_similarity(vec_i, vec_j)\n",
        "        # 計算誤差 e_WSL = |CosSim - WUP|\n",
        "        loss = torch.mean((cos_sim - target_wup) ** 2)\n",
        "        return loss\n",
        "\n",
        "# 2. 定義資料集類別，用於處理 CSV 讀取的資料\n",
        "class SemanticPairDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.encoded_i = tokenizer(dataframe['word_i'].tolist(), padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "        self.encoded_j = tokenizer(dataframe['word_j'].tolist(), padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "        self.wup_sim = torch.tensor(dataframe['wup_similarity'].values.astype(float), dtype=torch.float)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.wup_sim)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids_i\": self.encoded_i[\"input_ids\"][idx],\n",
        "            \"attention_mask_i\": self.encoded_i[\"attention_mask\"][idx],\n",
        "            \"input_ids_j\": self.encoded_j[\"input_ids\"][idx],\n",
        "            \"attention_mask_j\": self.encoded_j[\"attention_mask\"][idx],\n",
        "            \"wup_sim\": self.wup_sim[idx]\n",
        "        }\n",
        "\n",
        "def run_training(csv_path):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 初始化模型與 Tokenizer\n",
        "    model_name = \"roberta-base\"\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "    base_model = RobertaModel.from_pretrained(model_name)\n",
        "\n",
        "    # 4. LoRA 配置 (r=16, alpha=32)\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        target_modules=[\"query\", \"key\", \"value\", \"output.dense\"],\n",
        "        lora_dropout=0.1,\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(base_model, lora_config)\n",
        "    model.to(device)\n",
        "\n",
        "    # 5. 資料切分 (9:1 訓練集/測試集)\n",
        "    full_df = pd.read_csv(csv_path)\n",
        "    full_df = full_df.sample(frac=0.03, random_state=42) # 減少資料量以加速訓練和除錯\n",
        "    train_df = full_df.sample(frac=0.9, random_state=42) # 90% 訓練集\n",
        "    test_df = full_df.drop(train_df.index)               # 10% 測試集\n",
        "\n",
        "    # ==================================\n",
        "    print(summary(model))\n",
        "    print(f\"test size:\", test_df.shape)\n",
        "    print(f\"train size:\", train_df.shape)\n",
        "    print(train_df.describe())\n",
        "    print(test_df.describe())\n",
        "    # ==================================\n",
        "\n",
        "    train_loader = DataLoader(SemanticPairDataset(train_df, tokenizer=tokenizer), batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(SemanticPairDataset(test_df, tokenizer=tokenizer), batch_size=32)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "    criterion = MeSHSemanticLoss()\n",
        "\n",
        "    # 紀錄 Loss 用於輸出圖表\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    # Early Stopping 參數\n",
        "    patience = 3\n",
        "    min_delta = 0.0001 # 最小改善幅度\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    # 6. 訓練迴圈\n",
        "    num_epochs = 40 # 增加 epochs 數量以配合 Early Stopping\n",
        "    for epoch in range(num_epochs):\n",
        "      model.train() # 訓練模式\n",
        "      total_train_loss = 0\n",
        "      for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 直接從 batch 獲取資料並移至 device\n",
        "        emb_i = model(input_ids=batch[\"input_ids_i\"].to(device),\n",
        "                      attention_mask=batch[\"attention_mask_i\"].to(device)).last_hidden_state[:, 0, :]\n",
        "        emb_j = model(input_ids=batch[\"input_ids_j\"].to(device),\n",
        "                      attention_mask=batch[\"attention_mask_j\"].to(device)).last_hidden_state[:, 0, :]\n",
        "\n",
        "        loss = criterion(emb_i, emb_j, batch[\"wup_sim\"].to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "      # 驗證階段修正\n",
        "      model.eval()\n",
        "      total_val_loss = 0\n",
        "      with torch.no_grad():\n",
        "          for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n",
        "              emb_i = model(input_ids=batch[\"input_ids_i\"].to(device),\n",
        "                            attention_mask=batch[\"attention_mask_i\"].to(device)).last_hidden_state[:, 0, :]\n",
        "              emb_j = model(input_ids=batch[\"input_ids_j\"].to(device),\n",
        "                            attention_mask=batch[\"attention_mask_j\"].to(device)).last_hidden_state[:, 0, :]\n",
        "              total_val_loss += criterion(emb_i, emb_j, batch[\"wup_sim\"].to(device)).item()\n",
        "\n",
        "      avg_train = total_train_loss / len(train_loader)\n",
        "      avg_val = total_val_loss / len(val_loader)\n",
        "      train_losses.append(avg_train)\n",
        "      val_losses.append(avg_val)\n",
        "\n",
        "      print(f\"Epoch {epoch+1}: Train Loss={avg_train:.6f}, Test Loss={avg_val:.6f}\")\n",
        "\n",
        "      # Early Stopping 檢查\n",
        "      if avg_val < best_val_loss - min_delta:\n",
        "          best_val_loss = avg_val\n",
        "          patience_counter = 0\n",
        "          # 可以選擇在這裡保存最佳模型\n",
        "          model.save_pretrained(\"roberta_semantic_lora_best\")\n",
        "      else:\n",
        "          patience_counter += 1\n",
        "          print(f\"Early Stopping: Validation loss has not improved for {patience_counter} epochs.\")\n",
        "          if patience_counter >= patience:\n",
        "              print(f\"Early stopping triggered after {epoch+1} epochs. Best validation loss: {best_val_loss:.6f}\")\n",
        "              break\n",
        "\n",
        "    # 7. 繪製 Loss 曲線\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Val Loss')\n",
        "    plt.title('Training and Validation Semantic Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # 儲存 LoRA 適配器\n",
        "    model.save_pretrained(\"roberta_semantic_lora\")\n",
        "    print(\"LoRA Adapter saved to 'roberta_semantic_lora'.\")\n",
        "# 執行\n",
        "if __name__ == \"__main__\":\n",
        "    run_training('mesh_dataset.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "17h4YyiI7wp1"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}