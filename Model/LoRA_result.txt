每個類別將統一採樣至: 427 筆
train data size:  (8838, 2)
test data size:  (983, 2)
Running Experiment with RoBERTa + LoRA...
/tmp/ipython-input-2353705849.py:175: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  full_df = full_df.groupby('label').apply(
/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: 
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
tokenizer_config.json: 100%
 25.0/25.0 [00:00<00:00, 2.36kB/s]
vocab.json: 100%
 899k/899k [00:00<00:00, 5.80MB/s]
merges.txt: 100%
 456k/456k [00:00<00:00, 4.12MB/s]
tokenizer.json: 100%
 1.36M/1.36M [00:00<00:00, 21.0MB/s]
config.json: 100%
 481/481 [00:00<00:00, 55.6kB/s]
model.safetensors: 100%
 499M/499M [00:04<00:00, 87.6MB/s]
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
base model：

 ===============================================================================================
Layer (type:depth-idx)                                                 Param #
===============================================================================================
PeftModel                                                              --
├─LoraModel: 1-1                                                       --
│    └─RobertaModel: 2-1                                               --
│    │    └─RobertaEmbeddings: 3-1                                     (39,000,576)
│    │    └─RobertaEncoder: 3-2                                        (86,971,392)
│    │    └─RobertaPooler: 3-3                                         (590,592)
===============================================================================================
Total params: 126,562,560
Trainable params: 0
Non-trainable params: 126,562,560
===============================================================================================
classifer NN Head：

 =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Sequential                               --
├─Linear: 1-1                            17,687
=================================================================
Total params: 17,687
Trainable params: 17,687
Non-trainable params: 0
=================================================================
Epoch 1: 100%|██████████| 249/249 [04:27<00:00,  1.08s/it]
Epoch 1 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 1 Training Loss: 3.0321 Validation Loss: 2.8280
Epoch 2: 100%|██████████| 249/249 [04:38<00:00,  1.12s/it]
Epoch 2 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 2 Training Loss: 2.8026 Validation Loss: 2.6196
Epoch 3: 100%|██████████| 249/249 [04:37<00:00,  1.11s/it]
Epoch 3 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 3 Training Loss: 2.6804 Validation Loss: 2.5036
Epoch 4: 100%|██████████| 249/249 [04:38<00:00,  1.12s/it]
Epoch 4 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 4 Training Loss: 2.6126 Validation Loss: 2.4228
Epoch 5: 100%|██████████| 249/249 [04:37<00:00,  1.12s/it]
Epoch 5 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 5 Training Loss: 2.5573 Validation Loss: 2.3635
Epoch 6: 100%|██████████| 249/249 [04:38<00:00,  1.12s/it]
Epoch 6 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 6 Training Loss: 2.5128 Validation Loss: 2.3253
Epoch 7: 100%|██████████| 249/249 [04:37<00:00,  1.12s/it]
Epoch 7 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 7 Training Loss: 2.4774 Validation Loss: 2.2900
Epoch 8: 100%|██████████| 249/249 [04:38<00:00,  1.12s/it]
Epoch 8 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.05it/s]
Epoch 8 Training Loss: 2.4639 Validation Loss: 2.2721
Epoch 9: 100%|██████████| 249/249 [04:37<00:00,  1.11s/it]
Epoch 9 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 9 Training Loss: 2.4338 Validation Loss: 2.2602
Epoch 10: 100%|██████████| 249/249 [04:38<00:00,  1.12s/it]
Epoch 10 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 10 Training Loss: 2.4294 Validation Loss: 2.2502
Epoch 11: 100%|██████████| 249/249 [04:37<00:00,  1.12s/it]
Epoch 11 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 11 Training Loss: 2.4179 Validation Loss: 2.2346
Epoch 12: 100%|██████████| 249/249 [04:38<00:00,  1.12s/it]
Epoch 12 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 12 Training Loss: 2.4005 Validation Loss: 2.2236
Epoch 13: 100%|██████████| 249/249 [04:38<00:00,  1.12s/it]
Epoch 13 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 13 Training Loss: 2.3829 Validation Loss: 2.2170
Epoch 14: 100%|██████████| 249/249 [04:38<00:00,  1.12s/it]
Epoch 14 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.05it/s]
Epoch 14 Training Loss: 2.3780 Validation Loss: 2.2104
Epoch 15: 100%|██████████| 249/249 [04:38<00:00,  1.12s/it]
Epoch 15 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 15 Training Loss: 2.3705 Validation Loss: 2.1992
Epoch 16: 100%|██████████| 249/249 [04:38<00:00,  1.12s/it]
Epoch 16 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 16 Training Loss: 2.3761 Validation Loss: 2.2002
Epoch 17: 100%|██████████| 249/249 [04:38<00:00,  1.12s/it]
Epoch 17 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 17 Training Loss: 2.3669 Validation Loss: 2.1970
Epoch 18: 100%|██████████| 249/249 [04:38<00:00,  1.12s/it]
Epoch 18 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.05it/s]
Epoch 18 Training Loss: 2.3664 Validation Loss: 2.1947
Epoch 19: 100%|██████████| 249/249 [04:37<00:00,  1.12s/it]
Epoch 19 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 19 Training Loss: 2.3550 Validation Loss: 2.1899
Epoch 20: 100%|██████████| 249/249 [04:36<00:00,  1.11s/it]
Epoch 20 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.05it/s]
Epoch 20 Training Loss: 2.3647 Validation Loss: 2.1902
Epoch 21: 100%|██████████| 249/249 [04:36<00:00,  1.11s/it]
Epoch 21 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 21 Training Loss: 2.3475 Validation Loss: 2.1875
Epoch 22: 100%|██████████| 249/249 [04:38<00:00,  1.12s/it]
Epoch 22 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.05it/s]
Epoch 22 Training Loss: 2.3427 Validation Loss: 2.1794
Epoch 23: 100%|██████████| 249/249 [04:37<00:00,  1.12s/it]
Epoch 23 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 23 Training Loss: 2.3353 Validation Loss: 2.1802
Epoch 24: 100%|██████████| 249/249 [04:37<00:00,  1.12s/it]
Epoch 24 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.05it/s]
Epoch 24 Training Loss: 2.3413 Validation Loss: 2.1846
Epoch 25: 100%|██████████| 249/249 [04:37<00:00,  1.12s/it]
Epoch 25 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.05it/s]
Epoch 25 Training Loss: 2.3202 Validation Loss: 2.1767
Epoch 26: 100%|██████████| 249/249 [04:37<00:00,  1.12s/it]
Epoch 26 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.05it/s]
Epoch 26 Training Loss: 2.3239 Validation Loss: 2.1740
Epoch 27: 100%|██████████| 249/249 [04:38<00:00,  1.12s/it]
Epoch 27 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 27 Training Loss: 2.3114 Validation Loss: 2.1708
Epoch 28: 100%|██████████| 249/249 [04:37<00:00,  1.11s/it]
Epoch 28 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.05it/s]
Epoch 28 Training Loss: 2.3200 Validation Loss: 2.1731
Epoch 29: 100%|██████████| 249/249 [04:38<00:00,  1.12s/it]
Epoch 29 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 29 Training Loss: 2.3279 Validation Loss: 2.1728
Epoch 30: 100%|██████████| 249/249 [04:38<00:00,  1.12s/it]
Epoch 30 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.06it/s]
Epoch 30 Training Loss: 2.3351 Validation Loss: 2.1675
Epoch 31: 100%|██████████| 249/249 [04:32<00:00,  1.09s/it]
Epoch 31 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.07it/s]
Epoch 31 Training Loss: 2.3214 Validation Loss: 2.1745
Epoch 32: 100%|██████████| 249/249 [04:32<00:00,  1.10s/it]
Epoch 32 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.06it/s]
Epoch 32 Training Loss: 2.3195 Validation Loss: 2.1666
Epoch 33: 100%|██████████| 249/249 [04:32<00:00,  1.09s/it]
Epoch 33 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.06it/s]
Epoch 33 Training Loss: 2.3243 Validation Loss: 2.1630
Epoch 34: 100%|██████████| 249/249 [04:32<00:00,  1.10s/it]
Epoch 34 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.06it/s]
Epoch 34 Training Loss: 2.2941 Validation Loss: 2.1684
Epoch 35: 100%|██████████| 249/249 [04:36<00:00,  1.11s/it]
Epoch 35 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 35 Training Loss: 2.3149 Validation Loss: 2.1617
Epoch 36: 100%|██████████| 249/249 [04:37<00:00,  1.11s/it]
Epoch 36 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.05it/s]
Epoch 36 Training Loss: 2.3115 Validation Loss: 2.1567
Epoch 37: 100%|██████████| 249/249 [04:38<00:00,  1.12s/it]
Epoch 37 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 37 Training Loss: 2.3068 Validation Loss: 2.1607
Epoch 38: 100%|██████████| 249/249 [04:37<00:00,  1.11s/it]
Epoch 38 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.06it/s]
Epoch 38 Training Loss: 2.3205 Validation Loss: 2.1640
Epoch 39: 100%|██████████| 249/249 [04:32<00:00,  1.09s/it]
Epoch 39 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 39 Training Loss: 2.2975 Validation Loss: 2.1625
Epoch 40: 100%|██████████| 249/249 [04:37<00:00,  1.11s/it]
Epoch 40 Validation: 100%|██████████| 28/28 [00:26<00:00,  1.04it/s]
Epoch 40 Training Loss: 2.3155 Validation Loss: 2.1596

--- Final Evaluation Report ---
              precision    recall  f1-score   support

           0       0.19      0.19      0.19        42
           1       0.44      0.60      0.51        43
           2       0.47      0.63      0.54        43
           3       0.29      0.40      0.33        43
           4       0.27      0.30      0.29        43
           5       0.36      0.37      0.36        43
           6       0.46      0.42      0.44        43
           7       0.36      0.30      0.33        43
           8       0.47      0.55      0.51        42
           9       0.47      0.33      0.39        42
          10       0.46      0.40      0.42        43
          11       0.32      0.35      0.33        43
          12       0.44      0.40      0.42        42
          13       0.36      0.56      0.44        43
          14       0.32      0.35      0.33        43
          15       0.43      0.31      0.36        42
          16       0.33      0.26      0.29        43
          17       0.44      0.67      0.53        42
          18       0.48      0.47      0.47        43
          19       0.23      0.21      0.22        43
          20       0.42      0.40      0.41        43
          21       0.30      0.23      0.26        43
          22       0.00      0.00      0.00        43

    accuracy                           0.38       983
   macro avg       0.36      0.38      0.36       983
weighted avg       0.36      0.38      0.36       983

