{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPob18whr+ypzRaeBYaNZOw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Heng1222/Ohsumed_classification/blob/main/task1_pytorch_framework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXG9cQ6quH3u",
        "outputId": "c77b902a-6b46-4e4e-c23a-f4d0a2ac0d8a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.12/dist-packages (1.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sts8C2AGIxk9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b5e500f4-3bbb-4cd6-c99b-e6fc5f15421f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   label                                              title\n",
              "0      0  Haemophilus influenzae meningitis with prolong...\n",
              "1      0     Augmentation mentoplasty using Mersilene mesh.\n",
              "2      0  Multiple intracranial mucoceles associated wit...\n",
              "3      0  Replacement of an aortic valve cusp after neon...\n",
              "4      0  Mucosal intussusception to avoid ascending cho..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e079729b-61f8-4bb2-ab60-e731d8cd4acf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Haemophilus influenzae meningitis with prolong...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Augmentation mentoplasty using Mersilene mesh.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>Multiple intracranial mucoceles associated wit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>Replacement of an aortic valve cusp after neon...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Mucosal intussusception to avoid ascending cho...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e079729b-61f8-4bb2-ab60-e731d8cd4acf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e079729b-61f8-4bb2-ab60-e731d8cd4acf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e079729b-61f8-4bb2-ab60-e731d8cd4acf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"  # run_experiment('roberta-base', train_df, test_df, useLoRA = True)\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Augmentation mentoplasty using Mersilene mesh.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3105448943.py:156: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  full_df = full_df.groupby('label', group_keys=False).apply(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data size:  (5130, 2)\n",
            "test data size:  (571, 2)\n",
            "Running Experiment with Base RoBERTa...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================================================================================\n",
            "Layer (type:depth-idx)                                       Param #\n",
            "=====================================================================================\n",
            "RobertaModel                                                 --\n",
            "├─RobertaEmbeddings: 1-1                                     --\n",
            "│    └─Embedding: 2-1                                        (38,603,520)\n",
            "│    └─Embedding: 2-2                                        (394,752)\n",
            "│    └─Embedding: 2-3                                        (768)\n",
            "│    └─LayerNorm: 2-4                                        (1,536)\n",
            "│    └─Dropout: 2-5                                          --\n",
            "├─RobertaEncoder: 1-2                                        --\n",
            "│    └─ModuleList: 2-6                                       --\n",
            "│    │    └─RobertaLayer: 3-1                                (7,087,872)\n",
            "│    │    └─RobertaLayer: 3-2                                (7,087,872)\n",
            "│    │    └─RobertaLayer: 3-3                                (7,087,872)\n",
            "│    │    └─RobertaLayer: 3-4                                (7,087,872)\n",
            "│    │    └─RobertaLayer: 3-5                                (7,087,872)\n",
            "│    │    └─RobertaLayer: 3-6                                (7,087,872)\n",
            "│    │    └─RobertaLayer: 3-7                                (7,087,872)\n",
            "│    │    └─RobertaLayer: 3-8                                (7,087,872)\n",
            "│    │    └─RobertaLayer: 3-9                                (7,087,872)\n",
            "│    │    └─RobertaLayer: 3-10                               (7,087,872)\n",
            "│    │    └─RobertaLayer: 3-11                               (7,087,872)\n",
            "│    │    └─RobertaLayer: 3-12                               (7,087,872)\n",
            "├─RobertaPooler: 1-3                                         --\n",
            "│    └─Linear: 2-7                                           (590,592)\n",
            "│    └─Tanh: 2-8                                             --\n",
            "=====================================================================================\n",
            "Total params: 124,645,632\n",
            "Trainable params: 0\n",
            "Non-trainable params: 124,645,632\n",
            "=====================================================================================\n",
            "=================================================================\n",
            "Layer (type:depth-idx)                   Param #\n",
            "=================================================================\n",
            "Sequential                               --\n",
            "├─Linear: 1-1                            393,728\n",
            "├─ReLU: 1-2                              --\n",
            "├─Dropout: 1-3                           --\n",
            "├─Linear: 1-4                            11,799\n",
            "=================================================================\n",
            "Total params: 405,527\n",
            "Trainable params: 405,527\n",
            "Non-trainable params: 0\n",
            "=================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 145/145 [02:22<00:00,  1.01it/s]\n",
            "Epoch 1 Validation: 100%|██████████| 17/17 [00:13<00:00,  1.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Training Loss: 2.9396 Validation Loss: 2.9000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 145/145 [02:20<00:00,  1.03it/s]\n",
            "Epoch 2 Validation: 100%|██████████| 17/17 [00:13<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Training Loss: 2.8537 Validation Loss: 2.8908\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 145/145 [02:21<00:00,  1.03it/s]\n",
            "Epoch 3 Validation: 100%|██████████| 17/17 [00:13<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Training Loss: 2.8472 Validation Loss: 2.8891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 145/145 [02:20<00:00,  1.03it/s]\n",
            "Epoch 4 Validation: 100%|██████████| 17/17 [00:13<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Training Loss: 2.8251 Validation Loss: 2.8845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 145/145 [02:20<00:00,  1.03it/s]\n",
            "Epoch 5 Validation: 100%|██████████| 17/17 [00:13<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Training Loss: 2.8340 Validation Loss: 2.8786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 145/145 [02:20<00:00,  1.03it/s]\n",
            "Epoch 6 Validation: 100%|██████████| 17/17 [00:13<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Training Loss: 2.8139 Validation Loss: 2.8749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 145/145 [02:21<00:00,  1.03it/s]\n",
            "Epoch 7 Validation: 100%|██████████| 17/17 [00:13<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Training Loss: 2.8106 Validation Loss: 2.8740\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 145/145 [02:20<00:00,  1.03it/s]\n",
            "Epoch 8 Validation: 100%|██████████| 17/17 [00:13<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Training Loss: 2.8030 Validation Loss: 2.8666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 145/145 [02:20<00:00,  1.03it/s]\n",
            "Epoch 9 Validation: 100%|██████████| 17/17 [00:13<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Training Loss: 2.7905 Validation Loss: 2.8594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 145/145 [02:20<00:00,  1.03it/s]\n",
            "Epoch 10 Validation: 100%|██████████| 17/17 [00:13<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Training Loss: 2.7860 Validation Loss: 2.8485\n",
            "\n",
            "--- Final Evaluation Report ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        26\n",
            "           1       0.00      0.00      0.00        12\n",
            "           2       0.00      0.00      0.00         4\n",
            "           3       0.47      0.11      0.18        64\n",
            "           4       0.00      0.00      0.00        17\n",
            "           5       0.00      0.00      0.00        30\n",
            "           6       0.00      0.00      0.00         5\n",
            "           7       0.00      0.00      0.00        26\n",
            "           8       0.00      0.00      0.00         7\n",
            "           9       0.00      0.00      0.00        39\n",
            "          10       0.00      0.00      0.00        10\n",
            "          11       0.00      0.00      0.00        25\n",
            "          12       0.00      0.00      0.00        16\n",
            "          13       0.00      0.00      0.00        61\n",
            "          14       0.00      0.00      0.00        13\n",
            "          15       0.00      0.00      0.00        11\n",
            "          16       0.00      0.00      0.00        16\n",
            "          17       0.00      0.00      0.00        19\n",
            "          18       0.00      0.00      0.00         9\n",
            "          19       0.00      0.00      0.00        31\n",
            "          20       0.00      0.00      0.00        29\n",
            "          21       0.00      0.00      0.00         5\n",
            "          22       0.17      0.97      0.29        96\n",
            "\n",
            "    accuracy                           0.18       571\n",
            "   macro avg       0.03      0.05      0.02       571\n",
            "weighted avg       0.08      0.18      0.07       571\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaModel, RobertaTokenizer, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from peft import PeftModel, PeftConfig\n",
        "from torchinfo import summary\n",
        "\n",
        "# 1. 定義分類模型\n",
        "class RobertaClassifier(nn.Module):\n",
        "  def __init__(self, model_path_or_name, num_labels=23, freeze_backbone=True, useLoRA = False):\n",
        "    super(RobertaClassifier, self).__init__()\n",
        "    # 載入 RoBERTa\n",
        "    self.roberta = RobertaModel.from_pretrained(model_path_or_name)\n",
        "    # LoRA 掛載\n",
        "    if(useLoRA):\n",
        "      # config = PeftConfig.from_pretrained(\"ybelkada/opt-350m-lora\")\n",
        "      LoRA_folder = \"/content/roberta_semantic_lora\"\n",
        "      self.roberta = PeftModel.from_pretrained(self.roberta, LoRA_folder)\n",
        "    # 凍結 RoBERTa 參數\n",
        "    if freeze_backbone:\n",
        "      for param in self.roberta.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # 定義分類層 (NN Head)\n",
        "    # RoBERTa-base 的 hidden_size 是 768\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.Linear(768, 512),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(0.1),\n",
        "      nn.Linear(512, num_labels)\n",
        "    )\n",
        "\n",
        "    # summary model\n",
        "    print(\"base model：\\n\\n\",summary(self.roberta))\n",
        "    print(\"classifer NN Head：\\n\\n\", summary(self.classifier))\n",
        "\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    # 使用 CLS Token 的向量 (也可以換成 Mean Pooling)\n",
        "    cls_output = outputs.last_hidden_state[:, 0, :]\n",
        "    logits = self.classifier(cls_output)\n",
        "    return logits\n",
        "\n",
        "# 2. 資料集處理\n",
        "class TextDataset(Dataset):\n",
        "  def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "    self.encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_len, return_tensors=\"pt\")\n",
        "    self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return {\n",
        "      'input_ids': self.encodings['input_ids'][idx],\n",
        "      'attention_mask': self.encodings['attention_mask'][idx],\n",
        "      'labels': self.labels[idx]\n",
        "    }\n",
        "\n",
        "# 3. 訓練與評估主程式\n",
        "def run_experiment(model_name_or_path, train_df, test_df, num_epochs=10, useLoRA = False):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "  # 初始化\n",
        "  tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "  model = RobertaClassifier(model_name_or_path, num_labels=23, useLoRA = useLoRA)\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  # 測試集切割 9:1\n",
        "  train_df, val_df = train_test_split(\n",
        "    train_df,\n",
        "    test_size=0.1,       # 抽取 10% 作為測試集\n",
        "    random_state=42,\n",
        "    stratify=train_df['label']\n",
        "  )\n",
        "  train_loader = DataLoader(TextDataset(train_df['title'].tolist(), train_df['label'].tolist(), tokenizer), batch_size=32, shuffle=True)\n",
        "  val_loader =  DataLoader(TextDataset(val_df['title'].tolist(), val_df['label'].tolist(), tokenizer), batch_size=32)\n",
        "  test_loader = DataLoader(TextDataset(test_df['title'].tolist(), test_df['label'].tolist(), tokenizer), batch_size=32)\n",
        "\n",
        "  optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=1e-4)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # 訓練迴圈\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "      optimizer.zero_grad()\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "\n",
        "      logits = model(input_ids, attention_mask)\n",
        "      loss = criterion(logits, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_total_loss = 0\n",
        "    with torch.no_grad():\n",
        "      for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        val_loss = criterion(logits, labels)\n",
        "        val_total_loss += val_loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Training Loss: {total_loss/len(train_loader):.4f} Validation Loss: {val_total_loss/len(val_loader):.4f}\")\n",
        "\n",
        "  # test\n",
        "  model.eval()\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "  with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "\n",
        "      logits = model(input_ids, attention_mask)\n",
        "      preds = torch.argmax(logits, dim=1)\n",
        "      all_preds.extend(preds.cpu().numpy())\n",
        "      all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "  # 輸出詳細報表\n",
        "  print(\"\\n--- Final Evaluation Report ---\")\n",
        "  print(classification_report(all_labels, all_preds))\n",
        "  return all_labels, all_preds\n",
        "\n",
        "# 執行範例\n",
        "if __name__ == \"__main__\":\n",
        "  # 資料讀取\n",
        "  url = 'https://github.com/Heng1222/Ohsumed_classification/blob/main/classification_data/ohsumed_dataset.csv?raw=true'\n",
        "  full_df = pd.read_csv(url)\n",
        "  columns = ['label', 'title']\n",
        "  full_df = full_df[columns]\n",
        "\n",
        "  # 將分類標籤從字串轉換為數字\n",
        "  unique_labels = full_df['label'].unique()\n",
        "  label_to_id = {label: i for i, label in enumerate(sorted(unique_labels))}\n",
        "  # 將 'label' 欄位轉換為數字\n",
        "  full_df['label'] = full_df['label'].map(label_to_id)\n",
        "  display(full_df.head())\n",
        "  # 分層抽樣\n",
        "  full_df = full_df.groupby('label', group_keys=False).apply(\n",
        "    lambda x: x.sample(frac=0.1, random_state=42)\n",
        "  )\n",
        "  train_df, test_df = train_test_split(\n",
        "    full_df,\n",
        "    test_size=0.1,       # 抽取 10% 作為測試集\n",
        "    random_state=42,\n",
        "    stratify=full_df['label']\n",
        "  )\n",
        "  print(\"train data size: \", train_df.shape)\n",
        "  print(\"test data size: \", test_df.shape)\n",
        "\n",
        "  # 實驗 1: 使用 原版RoBERTa\n",
        "  print(\"Running Experiment with Base RoBERTa...\")\n",
        "  run_experiment('roberta-base', train_df, test_df)\n",
        "\n",
        "  # 實驗 2: 使用 MLM\n",
        "  # print(\"Running Experiment with Custom RoBERTa...\")\n",
        "  # run_experiment('./my_mlm_roberta_path', train_df, test_df)\n",
        "\n",
        "  # 實驗 3: 使用 LoRA\n",
        "  # print(\"Running Experiment with RoBERTa + LoRA...\")\n",
        "  # run_experiment('roberta-base', train_df, test_df, useLoRA = True)\n"
      ]
    }
  ]
}
